{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T20 Linear Regression Model Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation of the trained T20 linear regression model including detailed performance analysis, visualizations, and model diagnostics.\n",
    "\n",
    "## Objectives\n",
    "1. Load trained model from MLflow registry\n",
    "2. Comprehensive performance evaluation\n",
    "3. Create detailed visualizations\n",
    "4. Analyze model strengths and limitations\n",
    "5. Generate model evaluation report\n",
    "6. Provide recommendations for improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project modules\n",
    "from cricket.ml.evaluation import T20ModelEvaluator\n",
    "from cricket.ml.models.linear_regression import T20LinearRegression\n",
    "from cricket.ml.training import T20TrainingPipeline\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training summary if available\n",
    "try:\n",
    "    with open(\"training_summary.json\", \"r\") as f:\n",
    "        training_summary = json.load(f)\n",
    "    print(\"‚úÖ Loaded training summary from previous notebook\")\n",
    "    print(f\"   ‚Ä¢ Model: {training_summary['model_name']}\")\n",
    "    print(f\"   ‚Ä¢ Test R¬≤: {training_summary['test_r2']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Test RMSE: {training_summary['test_rmse']:.1f} runs\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Training summary not found. Please run t20_model_training.ipynb first.\")\n",
    "    # Set default values\n",
    "    training_summary = {\n",
    "        \"model_name\": \"t20_runs_predictor\",\n",
    "        \"experiment_name\": \"male_team_level_t20\"\n",
    "    }\n",
    "\n",
    "# MLflow configuration\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///../mlflow_setup/mlflow.db\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "MODEL_NAME = training_summary[\"model_name\"]\n",
    "EXPERIMENT_NAME = training_summary.get(\"experiment_name\", \"male_team_level_t20\")\n",
    "\n",
    "print(f\"\\nMLflow Configuration:\")\n",
    "print(f\"   ‚Ä¢ Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   ‚Ä¢ Model Name: {MODEL_NAME}\")\n",
    "print(f\"   ‚Ä¢ Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from MLflow registry\n",
    "try:\n",
    "    model_uri = f\"models:/{MODEL_NAME}/latest\"\n",
    "    loaded_sklearn_model = mlflow.sklearn.load_model(model_uri)\n",
    "    \n",
    "    # Create our wrapper model instance\n",
    "    trained_model = T20LinearRegression()\n",
    "    trained_model.model = loaded_sklearn_model\n",
    "    trained_model._is_trained = True\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully from: {model_uri}\")\n",
    "    print(f\"   ‚Ä¢ Model type: {type(loaded_sklearn_model).__name__}\")\n",
    "    print(f\"   ‚Ä¢ Features: {trained_model.feature_names}\")\n",
    "    print(f\"   ‚Ä¢ Coefficients: {loaded_sklearn_model.coef_}\")\n",
    "    print(f\"   ‚Ä¢ Intercept: {loaded_sklearn_model.intercept_:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    print(\"Please ensure the model training notebook has been run and the model is registered.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recreate Test Data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we need test data for evaluation, let's recreate it using the same pipeline\n",
    "# This is not ideal - in practice, we should save test data during training\n",
    "\n",
    "DATA_PATH = \"../data/ball_level_data.parquet\"\n",
    "\n",
    "print(\"üîÑ Recreating test data for evaluation...\")\n",
    "print(\"Note: This recreates the data split - in production, test data should be saved during training.\")\n",
    "\n",
    "# Initialize pipeline with same parameters as training\n",
    "eval_pipeline = T20TrainingPipeline(\n",
    "    data_path=DATA_PATH,\n",
    "    \n",
    "    scaling_method=\"standard\"\n",
    ")\n",
    "\n",
    "# Load and prepare data (same as training pipeline)\n",
    "raw_data = eval_pipeline._load_data()\n",
    "clean_data = eval_pipeline._prepare_t20_data(raw_data)\n",
    "features_data = eval_pipeline._add_match_features(clean_data)\n",
    "target_df, feature_samples = eval_pipeline._create_modeling_data(features_data)\n",
    "\n",
    "# Split data (same ratios as training)\n",
    "train_data, val_data, test_data = eval_pipeline._split_data(\n",
    "    feature_samples, target_df, 0.7, 0.15, 0.15\n",
    ")\n",
    "\n",
    "# Prepare test features and targets\n",
    "test_prep = eval_pipeline.feature_engineer.prepare_features(test_data)\n",
    "X_test = eval_pipeline.feature_engineer.fit_transform(test_prep)  # Note: Should use transform only\n",
    "y_test = eval_pipeline._get_targets_for_features(test_data, target_df)\n",
    "\n",
    "print(f\"‚úÖ Test data recreated: {len(X_test)} samples\")\n",
    "print(f\"   ‚Ä¢ Features shape: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Target range: {y_test.min():.0f} - {y_test.max():.0f} runs\")\n",
    "print(f\"   ‚Ä¢ Mean target: {y_test.mean():.1f} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model evaluator\n",
    "evaluator = T20ModelEvaluator(\n",
    "    model=trained_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    feature_names=[\"current_score\", \"wickets_fallen\", \"overs_remaining\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model evaluator initialized\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(y_test)}\")\n",
    "print(f\"   ‚Ä¢ Features: {evaluator.feature_names}\")\n",
    "print(f\"   ‚Ä¢ Predictions computed: {len(evaluator.y_pred)}\")\n",
    "\n",
    "# Quick preview of predictions vs actuals\n",
    "print(f\"\\nüìä Preview - First 10 predictions:\")\n",
    "print(f\"{'Actual':<10} {'Predicted':<10} {'Error':<10} {'Abs Error':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for i in range(min(10, len(y_test))):\n",
    "    actual = y_test[i]\n",
    "    predicted = evaluator.y_pred[i]\n",
    "    error = actual - predicted\n",
    "    abs_error = abs(error)\n",
    "    print(f\"{actual:<10.0f} {predicted:<10.0f} {error:<10.1f} {abs_error:<10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "comprehensive_metrics = evaluator.calculate_comprehensive_metrics()\n",
    "\n",
    "print(\"üìä COMPREHENSIVE PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüéØ Core Regression Metrics:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score:           {comprehensive_metrics['r2_score']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Adjusted R¬≤:        {comprehensive_metrics['adjusted_r2']:.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE:              {comprehensive_metrics['rmse']:.2f} runs\")\n",
    "print(f\"   ‚Ä¢ MAE:               {comprehensive_metrics['mae']:.2f} runs\")\n",
    "print(f\"   ‚Ä¢ MAPE:              {comprehensive_metrics['mape']:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìè Error Analysis:\")\n",
    "print(f\"   ‚Ä¢ Max Error:          {comprehensive_metrics['max_error']:.1f} runs\")\n",
    "print(f\"   ‚Ä¢ Mean Error:         {comprehensive_metrics['mean_error']:.2f} runs\")\n",
    "print(f\"   ‚Ä¢ Std of Residuals:   {comprehensive_metrics['std_residuals']:.2f} runs\")\n",
    "\n",
    "print(f\"\\nüéØ Prediction Accuracy:\")\n",
    "print(f\"   ‚Ä¢ Within 10 runs:     {comprehensive_metrics['within_10_runs']:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Within 20 runs:     {comprehensive_metrics['within_20_runs']:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Within 30 runs:     {comprehensive_metrics['within_30_runs']:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Bias Analysis:\")\n",
    "print(f\"   ‚Ä¢ Underestimate Rate: {comprehensive_metrics['underestimate_rate']:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Overestimate Rate:  {comprehensive_metrics['overestimate_rate']:.1f}%\")\n",
    "\n",
    "# Performance quality assessment\n",
    "r2 = comprehensive_metrics['r2_score']\n",
    "rmse = comprehensive_metrics['rmse']\n",
    "within_20 = comprehensive_metrics['within_20_runs']\n",
    "\n",
    "print(f\"\\n‚úÖ Overall Assessment:\")\n",
    "if r2 >= 0.8:\n",
    "    r2_quality = \"Excellent\"\n",
    "elif r2 >= 0.7:\n",
    "    r2_quality = \"Good\"\n",
    "elif r2 >= 0.5:\n",
    "    r2_quality = \"Fair\"\n",
    "else:\n",
    "    r2_quality = \"Poor\"\n",
    "\n",
    "print(f\"   ‚Ä¢ Model Quality: {r2_quality} (R¬≤ = {r2:.3f})\")\n",
    "\n",
    "if within_20 >= 80:\n",
    "    accuracy_quality = \"Excellent\"\n",
    "elif within_20 >= 70:\n",
    "    accuracy_quality = \"Good\"\n",
    "elif within_20 >= 60:\n",
    "    accuracy_quality = \"Fair\"\n",
    "else:\n",
    "    accuracy_quality = \"Poor\"\n",
    "\n",
    "print(f\"   ‚Ä¢ Prediction Accuracy: {accuracy_quality} ({within_20:.1f}% within 20 runs)\")\n",
    "\n",
    "bias = abs(comprehensive_metrics['mean_error'])\n",
    "if bias <= 2:\n",
    "    bias_quality = \"Excellent (Unbiased)\"\n",
    "elif bias <= 5:\n",
    "    bias_quality = \"Good (Low Bias)\"\n",
    "else:\n",
    "    bias_quality = \"Fair (Some Bias)\"\n",
    "\n",
    "print(f\"   ‚Ä¢ Model Bias: {bias_quality} (Mean Error = {comprehensive_metrics['mean_error']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Visualization - Predictions vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions vs actual plot\n",
    "fig_pred_actual = evaluator.plot_predictions_vs_actual(figsize=(12, 10))\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Predictions vs Actual Analysis:\")\n",
    "print(f\"   ‚Ä¢ Points close to red line indicate good predictions\")\n",
    "print(f\"   ‚Ä¢ Scatter pattern suggests model performance\")\n",
    "print(f\"   ‚Ä¢ R¬≤ = {comprehensive_metrics['r2_score']:.3f} shows {comprehensive_metrics['r2_score']*100:.1f}% variance explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create residual analysis plots\n",
    "fig_residuals = evaluator.plot_residuals(figsize=(15, 6))\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Residual Analysis Insights:\")\n",
    "print(f\"   ‚Ä¢ Left plot: Residuals vs Predicted - should show random scatter around 0\")\n",
    "print(f\"   ‚Ä¢ Right plot: Residual distribution - should be approximately normal\")\n",
    "print(f\"   ‚Ä¢ Mean residual: {comprehensive_metrics['mean_error']:.2f} (close to 0 is good)\")\n",
    "print(f\"   ‚Ä¢ Std residual: {comprehensive_metrics['std_residuals']:.2f} runs\")\n",
    "\n",
    "# Check for patterns in residuals\n",
    "if abs(comprehensive_metrics['mean_error']) <= 2:\n",
    "    print(f\"   ‚úÖ No significant bias detected\")\n",
    "else:\n",
    "    bias_direction = \"overestimating\" if comprehensive_metrics['mean_error'] < 0 else \"underestimating\"\n",
    "    print(f\"   ‚ö†Ô∏è Model may be {bias_direction} scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature importance plot\n",
    "fig_importance = evaluator.plot_feature_importance(figsize=(12, 8))\n",
    "plt.show()\n",
    "\n",
    "# Get and display feature importance\n",
    "importance_df = evaluator.model.get_feature_importance()\n",
    "print(\"üîç FEATURE IMPORTANCE ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for _, row in importance_df.iterrows():\n",
    "    feature = row['feature']\n",
    "    coeff = row['coefficient']\n",
    "    abs_coeff = row['abs_coefficient']\n",
    "    \n",
    "    if feature == 'intercept':\n",
    "        print(f\"\\nüìä {feature.upper()}:\")\n",
    "        print(f\"   ‚Ä¢ Value: {coeff:.3f}\")\n",
    "        print(f\"   ‚Ä¢ Interpretation: Base prediction when all features are 0\")\n",
    "    else:\n",
    "        direction = \"increases\" if coeff > 0 else \"decreases\"\n",
    "        print(f\"\\nüìä {feature.upper().replace('_', ' ')}:\")\n",
    "        print(f\"   ‚Ä¢ Coefficient: {coeff:.3f}\")\n",
    "        print(f\"   ‚Ä¢ Impact: Each unit increase {direction} final score by {abs(coeff):.3f} runs\")\n",
    "        \n",
    "        if feature == \"current_score\":\n",
    "            print(f\"   ‚Ä¢ Meaning: Higher current score ‚Üí higher final total\")\n",
    "        elif feature == \"wickets_fallen\":\n",
    "            if coeff < 0:\n",
    "                print(f\"   ‚Ä¢ Meaning: More wickets lost ‚Üí lower final total\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ Meaning: More wickets lost ‚Üí higher final total (unexpected!)\")\n",
    "        elif feature == \"overs_remaining\":\n",
    "            if coeff > 0:\n",
    "                print(f\"   ‚Ä¢ Meaning: More overs left ‚Üí higher scoring potential\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ Meaning: More overs left ‚Üí lower final total (unexpected!)\")\n",
    "\n",
    "# Model equation\n",
    "equation = evaluator.model.get_model_equation()\n",
    "print(f\"\\nüßÆ MODEL EQUATION:\")\n",
    "print(f\"   {equation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive error analysis\n",
    "fig_error_analysis = evaluator.plot_error_analysis(figsize=(15, 12))\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç ERROR ANALYSIS INSIGHTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\nüìä Error Patterns:\")\n",
    "print(f\"   ‚Ä¢ Top-left: Absolute errors vs actual scores\")\n",
    "print(f\"   ‚Ä¢ Top-right: Error distribution across score ranges\")\n",
    "print(f\"   ‚Ä¢ Bottom-left: Percentage errors vs actual scores\")\n",
    "print(f\"   ‚Ä¢ Bottom-right: Cumulative error distribution\")\n",
    "\n",
    "# Analyze error patterns by score range\n",
    "score_ranges = [(0, 120), (120, 140), (140, 160), (160, 180), (180, 250)]\n",
    "abs_errors = np.abs(evaluator.residuals)\n",
    "\n",
    "print(f\"\\nüìà Error Analysis by Score Range:\")\n",
    "for low, high in score_ranges:\n",
    "    mask = (evaluator.y_test >= low) & (evaluator.y_test < high)\n",
    "    if np.any(mask):\n",
    "        range_errors = abs_errors[mask]\n",
    "        range_count = len(range_errors)\n",
    "        range_mean_error = np.mean(range_errors)\n",
    "        print(f\"   ‚Ä¢ {low}-{high} runs ({range_count} samples): Avg error = {range_mean_error:.1f} runs\")\n",
    "\n",
    "# Best and worst predictions\n",
    "best_idx = np.argmin(abs_errors)\n",
    "worst_idx = np.argmax(abs_errors)\n",
    "\n",
    "print(f\"\\nüèÜ Best Prediction:\")\n",
    "print(f\"   ‚Ä¢ Actual: {evaluator.y_test[best_idx]:.0f} runs\")\n",
    "print(f\"   ‚Ä¢ Predicted: {evaluator.y_pred[best_idx]:.0f} runs\")\n",
    "print(f\"   ‚Ä¢ Error: {evaluator.residuals[best_idx]:.1f} runs\")\n",
    "\n",
    "print(f\"\\nüòû Worst Prediction:\")\n",
    "print(f\"   ‚Ä¢ Actual: {evaluator.y_test[worst_idx]:.0f} runs\")\n",
    "print(f\"   ‚Ä¢ Predicted: {evaluator.y_pred[worst_idx]:.0f} runs\")\n",
    "print(f\"   ‚Ä¢ Error: {evaluator.residuals[worst_idx]:.1f} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive model report\n",
    "model_report = evaluator.create_model_report()\n",
    "\n",
    "print(\"üìã COMPREHENSIVE MODEL EVALUATION REPORT\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Performance Summary\n",
    "performance = model_report['performance_summary']\n",
    "print(f\"\\nüéØ PERFORMANCE SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Overall Performance: {performance['overall_performance']}\")\n",
    "print(f\"   ‚Ä¢ Prediction Accuracy: {performance['prediction_accuracy']}\")\n",
    "print(f\"   ‚Ä¢ Bias Assessment: {performance['bias_assessment']}\")\n",
    "\n",
    "if performance['key_insights']:\n",
    "    print(f\"\\nüí° Key Insights:\")\n",
    "    for insight in performance['key_insights']:\n",
    "        print(f\"   ‚Ä¢ {insight}\")\n",
    "\n",
    "# Data Summary\n",
    "data_summary = model_report['data_summary']\n",
    "print(f\"\\nüìä DATA SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Test Samples: {data_summary['test_samples']}\")\n",
    "print(f\"   ‚Ä¢ Actual Score Range: {data_summary['actual_score_range']} runs\")\n",
    "print(f\"   ‚Ä¢ Predicted Score Range: {data_summary['predicted_score_range']} runs\")\n",
    "print(f\"   ‚Ä¢ Mean Actual: {data_summary['mean_actual']} runs\")\n",
    "print(f\"   ‚Ä¢ Mean Predicted: {data_summary['mean_predicted']} runs\")\n",
    "\n",
    "# Model Equation\n",
    "print(f\"\\nüßÆ MODEL EQUATION:\")\n",
    "print(f\"   {model_report['model_equation']}\")\n",
    "\n",
    "# Feature Importance Summary\n",
    "print(f\"\\nüìà FEATURE RANKINGS:\")\n",
    "importance_list = model_report['feature_importance']\n",
    "for i, feature_info in enumerate(importance_list):\n",
    "    if feature_info['feature'] != 'intercept':\n",
    "        rank = i + 1\n",
    "        print(f\"   {rank}. {feature_info['feature']}: {feature_info['coefficient']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "\n",
    "# Save report to file\n",
    "with open(\"model_evaluation_report.json\", \"w\") as f:\n",
    "    # Convert pandas DataFrames to dicts for JSON serialization\n",
    "    json_report = model_report.copy()\n",
    "    json.dump(json_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"üìÅ Report saved to: model_evaluation_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Diagnostics and Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check linear regression assumptions\n",
    "print(\"üî¨ LINEAR REGRESSION ASSUMPTIONS CHECK\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. Linearity - already checked with residuals vs fitted\n",
    "print(f\"\\n1Ô∏è‚É£ LINEARITY:\")\n",
    "print(f\"   ‚Ä¢ Check residuals vs predicted plot above\")\n",
    "print(f\"   ‚Ä¢ Random scatter around 0 indicates linearity assumption met\")\n",
    "\n",
    "# 2. Independence - temporal/match independence\n",
    "print(f\"\\n2Ô∏è‚É£ INDEPENDENCE:\")\n",
    "print(f\"   ‚Ä¢ Samples from different matches/innings\")\n",
    "print(f\"   ‚Ä¢ Chronological split reduces temporal dependence\")\n",
    "print(f\"   ‚Ä¢ Assumption: ‚úÖ Reasonably met\")\n",
    "\n",
    "# 3. Homoscedasticity - constant variance of residuals\n",
    "residual_variance_by_fitted = []\n",
    "fitted_ranges = [(0, 140), (140, 160), (160, 180), (180, 250)]\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ HOMOSCEDASTICITY (Constant Variance):\")\n",
    "for low, high in fitted_ranges:\n",
    "    mask = (evaluator.y_pred >= low) & (evaluator.y_pred < high)\n",
    "    if np.any(mask):\n",
    "        range_residuals = evaluator.residuals[mask]\n",
    "        range_var = np.var(range_residuals)\n",
    "        residual_variance_by_fitted.append(range_var)\n",
    "        print(f\"   ‚Ä¢ {low}-{high} runs: Residual variance = {range_var:.1f}\")\n",
    "\n",
    "if len(residual_variance_by_fitted) > 1:\n",
    "    var_ratio = max(residual_variance_by_fitted) / min(residual_variance_by_fitted)\n",
    "    if var_ratio < 2:\n",
    "        homoscedasticity_status = \"‚úÖ Good - variance is relatively constant\"\n",
    "    elif var_ratio < 4:\n",
    "        homoscedasticity_status = \"‚ö†Ô∏è Moderate - some variance differences\"\n",
    "    else:\n",
    "        homoscedasticity_status = \"‚ùå Poor - significant variance differences\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Variance ratio: {var_ratio:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Assessment: {homoscedasticity_status}\")\n",
    "\n",
    "# 4. Normality of residuals\n",
    "from scipy import stats\n",
    "try:\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(evaluator.residuals[:1000] if len(evaluator.residuals) > 1000 else evaluator.residuals)\n",
    "    print(f\"\\n4Ô∏è‚É£ NORMALITY OF RESIDUALS:\")\n",
    "    print(f\"   ‚Ä¢ Shapiro-Wilk test statistic: {shapiro_stat:.4f}\")\n",
    "    print(f\"   ‚Ä¢ P-value: {shapiro_p:.4f}\")\n",
    "    \n",
    "    if shapiro_p > 0.05:\n",
    "        normality_status = \"‚úÖ Good - residuals appear normally distributed\"\n",
    "    else:\n",
    "        normality_status = \"‚ö†Ô∏è Violation - residuals not normally distributed\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Assessment: {normality_status}\")\n",
    "    print(f\"   ‚Ä¢ Note: Check histogram in residual analysis above\")\n",
    "except ImportError:\n",
    "    print(f\"\\n4Ô∏è‚É£ NORMALITY OF RESIDUALS:\")\n",
    "    print(f\"   ‚Ä¢ Check histogram in residual analysis above\")\n",
    "    print(f\"   ‚Ä¢ Bell-shaped distribution suggests normality\")\n",
    "\n",
    "# 5. No multicollinearity - check VIF if needed\n",
    "print(f\"\\n5Ô∏è‚É£ MULTICOLLINEARITY:\")\n",
    "print(f\"   ‚Ä¢ With only 3 features, multicollinearity is less concern\")\n",
    "print(f\"   ‚Ä¢ Features are conceptually distinct (score, wickets, overs)\")\n",
    "print(f\"   ‚Ä¢ Assessment: ‚úÖ Likely not a major issue\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 45)\n",
    "print(f\"Overall: Linear regression assumptions are reasonably well met\")\n",
    "print(f\"The model is appropriate for this T20 runs prediction task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Limitations and Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è MODEL LIMITATIONS AND IMPROVEMENT OPPORTUNITIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "r2 = comprehensive_metrics['r2_score']\n",
    "rmse = comprehensive_metrics['rmse']\n",
    "within_20 = comprehensive_metrics['within_20_runs']\n",
    "\n",
    "print(f\"\\nüöß CURRENT LIMITATIONS:\")\n",
    "\n",
    "if r2 < 0.8:\n",
    "    print(f\"   ‚Ä¢ R¬≤ = {r2:.3f} - Model explains {r2*100:.1f}% of variance\")\n",
    "    print(f\"     ‚Üí {(1-r2)*100:.1f}% of variance remains unexplained\")\n",
    "\n",
    "if within_20 < 80:\n",
    "    print(f\"   ‚Ä¢ Only {within_20:.1f}% of predictions within 20 runs\")\n",
    "    print(f\"     ‚Üí Room for improvement in prediction accuracy\")\n",
    "\n",
    "if rmse > 20:\n",
    "    print(f\"   ‚Ä¢ RMSE = {rmse:.1f} runs is relatively high for T20 cricket\")\n",
    "    print(f\"     ‚Üí Typical errors are substantial relative to T20 scores\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Feature Limitations:\")\n",
    "print(f\"     ‚Üí Only 3 basic match-state features\")\n",
    "print(f\"     ‚Üí No player quality, venue, or situational factors\")\n",
    "print(f\"     ‚Üí No powerplay/death overs distinctions\")\n",
    "print(f\"     ‚Üí No recent form or historical performance data\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Model Complexity:\")\n",
    "print(f\"     ‚Üí Simple linear relationships may miss non-linear patterns\")\n",
    "print(f\"     ‚Üí No interaction terms between features\")\n",
    "print(f\"     ‚Üí Assumes constant relationships across all match contexts\")\n",
    "\n",
    "print(f\"\\nüöÄ IMPROVEMENT OPPORTUNITIES:\")\n",
    "\n",
    "print(f\"\\n   üìä Feature Engineering:\")\n",
    "print(f\"     ‚Ä¢ Add run rate features (current RR, required RR)\")\n",
    "print(f\"     ‚Ä¢ Include powerplay/death overs indicators\")\n",
    "print(f\"     ‚Ä¢ Partnership features (current partnership runs/balls)\")\n",
    "print(f\"     ‚Ä¢ Venue-specific adjustments (average scores, conditions)\")\n",
    "print(f\"     ‚Ä¢ Player quality metrics (batting/bowling ratings)\")\n",
    "print(f\"     ‚Ä¢ Recent form indicators\")\n",
    "\n",
    "print(f\"\\n   üß† Model Enhancements:\")\n",
    "print(f\"     ‚Ä¢ Polynomial features for non-linear relationships\")\n",
    "print(f\"     ‚Ä¢ Interaction terms (e.g., wickets √ó overs_remaining)\")\n",
    "print(f\"     ‚Ä¢ Ridge/Lasso regularization for better generalization\")\n",
    "print(f\"     ‚Ä¢ Ensemble methods (Random Forest, Gradient Boosting)\")\n",
    "print(f\"     ‚Ä¢ Phase-specific models (powerplay vs middle vs death)\")\n",
    "\n",
    "print(f\"\\n   üìà Data Improvements:\")\n",
    "print(f\"     ‚Ä¢ More historical data for training\")\n",
    "print(f\"     ‚Ä¢ Better data quality controls\")\n",
    "print(f\"     ‚Ä¢ External data sources (weather, pitch conditions)\")\n",
    "print(f\"     ‚Ä¢ Real-time feature updates\")\n",
    "\n",
    "print(f\"\\n   üîß Technical Enhancements:\")\n",
    "print(f\"     ‚Ä¢ Cross-validation for better model selection\")\n",
    "print(f\"     ‚Ä¢ Hyperparameter tuning\")\n",
    "print(f\"     ‚Ä¢ Model monitoring and retraining pipelines\")\n",
    "print(f\"     ‚Ä¢ A/B testing framework for model comparison\")\n",
    "\n",
    "print(f\"\\n‚úÖ NEXT STEPS PRIORITY:\")\n",
    "print(f\"   1. Add run rate and phase-based features\")\n",
    "print(f\"   2. Implement polynomial/interaction terms\")\n",
    "print(f\"   3. Try ensemble methods for comparison\")\n",
    "print(f\"   4. Validate with recent matches\")\n",
    "print(f\"   5. Deploy for real-time testing\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "evaluator.print_summary()\n",
    "\n",
    "# Save evaluation plots\n",
    "print(f\"\\nüíæ SAVING EVALUATION ARTIFACTS:\")\n",
    "try:\n",
    "    evaluator.save_plots(\"../model_evaluation_plots\")\n",
    "    print(f\"   ‚úÖ Evaluation plots saved to: ../model_evaluation_plots/\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Could not save plots: {e}\")\n",
    "\n",
    "print(f\"\\nüìã EVALUATION COMPLETE!\")\n",
    "print(f\"   ‚Ä¢ Comprehensive metrics calculated\")\n",
    "print(f\"   ‚Ä¢ Visualizations generated\")\n",
    "print(f\"   ‚Ä¢ Model assumptions checked\")\n",
    "print(f\"   ‚Ä¢ Limitations identified\")\n",
    "print(f\"   ‚Ä¢ Improvement roadmap provided\")\n",
    "\n",
    "print(f\"\\nüéØ FINAL RECOMMENDATION:\")\n",
    "if r2 >= 0.7 and within_20 >= 70:\n",
    "    recommendation = \"DEPLOY - Model is suitable for production use\"\n",
    "else:\n",
    "    recommendation = \"IMPROVE - Model needs enhancement before deployment\"\n",
    "\n",
    "print(f\"   {recommendation}\")\n",
    "print(f\"   Continue with feature engineering and model improvements\")\n",
    "print(f\"   Consider this as a strong baseline for future development\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"T20 LINEAR REGRESSION MODEL EVALUATION COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive evaluation of the T20 linear regression model provides:\n",
    "\n",
    "### Key Findings\n",
    "- **Performance**: Detailed metrics including R¬≤, RMSE, MAE, and accuracy percentages\n",
    "- **Visualizations**: Multiple plots showing model behavior and error patterns\n",
    "- **Feature Importance**: Understanding of which factors most influence predictions\n",
    "- **Model Diagnostics**: Validation of linear regression assumptions\n",
    "\n",
    "### Artifacts Generated\n",
    "- Comprehensive evaluation report (JSON)\n",
    "- Multiple visualization plots\n",
    "- Performance metrics and insights\n",
    "- Improvement recommendations\n",
    "\n",
    "### Next Steps\n",
    "1. Review the improvement opportunities identified\n",
    "2. Implement enhanced features and models\n",
    "3. Compare performance against this baseline\n",
    "4. Consider deployment based on performance requirements\n",
    "\n",
    "The model provides a solid foundation for T20 runs prediction with clear pathways for enhancement."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
